import pandas as pd
import itertools
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, ConcatDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import numpy as np
import os
import matplotlib
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import seaborn as sns
from scipy.stats import pearsonr, spearmanr

plt.rcParams['font.family'] = 'Malgun Gothic'  # Windowsìš© í•œê¸€ í°íŠ¸
plt.rcParams['axes.unicode_minus'] = False     # ìŒìˆ˜ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
matplotlib.use('Agg')

################################################################################################
####################################### Data load ######################################
################################################################################################
df = pd.read_csv('./Care_Duration_Prediction_Model/Data_Preprocessing/Regular_Expression/final_data.csv')
print(df.describe())
print(df.info())
print(df.columns)
print(df.head())
print('*'*50)
print('*'*50)
print()
################################################################################################
####################################### Create data set ######################################
################################################################################################
genders = ['ì„±ë³„_ë‚¨ì', 'ì„±ë³„_ì—¬ì']
surgeries = ['ìˆ˜ìˆ ì—¬ë¶€_ì˜ˆ', 'ìˆ˜ìˆ ì—¬ë¶€_ì•„ë‹ˆì˜¤']
ages = ['ì—°ë ¹ëŒ€_30ì„¸ë¯¸ë§Œ', 'ì—°ë ¹ëŒ€_30-39ì„¸', 'ì—°ë ¹ëŒ€_40-49ì„¸', 'ì—°ë ¹ëŒ€_50-59ì„¸', 'ì—°ë ¹ëŒ€_60ì„¸ì´ìƒ']
regions = ['ì§€ì—­ë³¸ë¶€_ì„œìš¸ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ë¶€ì‚°ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ëŒ€êµ¬ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ê²½ì¸ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ê´‘ì£¼ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ëŒ€ì „ì§€ì—­']

combinations = list(itertools.product(genders, surgeries, ages, regions))

data_set = []

for _, row in df.iterrows():
    disease = row['ë³‘ëª…']
    for gender, surgery, age, region in combinations:
        input_row = [disease, gender, surgery, age, region]
        output_row = [
            row.get(gender, None),
            row.get(surgery, None),
            row.get(age, None),
            row.get(region, None)
        ]
        data_set.append(input_row + output_row)
data_df = pd.DataFrame(data_set, columns=['ë³‘ëª…', 'ì„±ë³„', 'ìˆ˜ìˆ ì—¬ë¶€', 'ì—°ë ¹ëŒ€', 'ì§€ì—­ë³¸ë¶€','ì„±ë³„_ìš”ì–‘ì¼', 'ìˆ˜ìˆ ì—¬ë¶€_ìš”ì–‘ì¼', 'ì—°ë ¹ëŒ€_ìš”ì–‘ì¼', 'ì§€ì—­ë³¸ë¶€_ìš”ì–‘ì¼'])
print(data_df.head())
print('*'*50)
print('*'*50)
print()
################################################################################################
####################################### Data Preprocessing ######################################
################################################################################################

# ê²°ì¸¡ì¹˜(NaN)ê°€ ìˆëŠ” í–‰ ì œê±°
data_df = data_df.dropna().reset_index(drop=True)
print(data_df.isna().sum())
# ë³µì‚¬ë³¸ ìœ ì§€
processed_df = data_df.copy()

# Label Encoding: ë³‘ëª…
le = LabelEncoder()
processed_df['ë³‘ëª…'] = le.fit_transform(processed_df['ë³‘ëª…'])

# One-Hot Encoding: ì„±ë³„, ìˆ˜ìˆ ì—¬ë¶€, ì—°ë ¹ëŒ€, ì§€ì—­ë³¸ë¶€
processed_df = processed_df.replace({True: 1, False: 0})
categorical_cols = ['ì„±ë³„', 'ìˆ˜ìˆ ì—¬ë¶€', 'ì—°ë ¹ëŒ€', 'ì§€ì—­ë³¸ë¶€']
processed_df = pd.get_dummies(processed_df, columns=categorical_cols)

# ì •ê·œí™”: ì¶œë ¥ê°’ 4ê°œ
scaler = StandardScaler()
output_cols = ['ì„±ë³„_ìš”ì–‘ì¼', 'ìˆ˜ìˆ ì—¬ë¶€_ìš”ì–‘ì¼', 'ì—°ë ¹ëŒ€_ìš”ì–‘ì¼', 'ì§€ì—­ë³¸ë¶€_ìš”ì–‘ì¼']
processed_df[output_cols] = scaler.fit_transform(processed_df[output_cols])

# ìµœì¢… ì…ë ¥ê³¼ ì¶œë ¥ ë¶„ë¦¬
X = processed_df.drop(columns=output_cols)
y = processed_df[output_cols]

print(X.head())
print(y.head())

# Train/valid/test split (60/20/20 ë¹„ìœ¨ ê¸°ì¤€)
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, stratify=X['ë³‘ëª…'], random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, stratify=X_trainval['ë³‘ëª…'], random_state=42)

print(f'X_train_shape: {X_train.shape}')
print(f'X_val_shape: {X_val.shape}')
print(f'X_test_shape: {X_test.shape}')
print(f'y_train_shape: {y_train.shape}')
print(f'y_val_shape: {y_val.shape}')
print(f'y_test_shape: {y_test.shape}')
print('*'*50)
print('*'*50)
print()

# ë³‘ëª… ì¸ë±ìŠ¤ë§Œ ë”°ë¡œ ì¶”ì¶œ (ì„ë² ë”© ì…ë ¥ìš©)
disease_train = torch.tensor(X_train['ë³‘ëª…'].values, dtype=torch.long)
disease_val = torch.tensor(X_val['ë³‘ëª…'].values, dtype=torch.long)
disease_test = torch.tensor(X_test['ë³‘ëª…'].values, dtype=torch.long)

# ë³‘ëª… ì™¸ ë‚˜ë¨¸ì§€ feature ì¶”ì¶œ
X_train_tensor = torch.tensor(X_train.drop(columns=['ë³‘ëª…']).astype(np.float32).values)
X_val_tensor = torch.tensor(X_val.drop(columns=['ë³‘ëª…']).astype(np.float32).values)
X_test_tensor = torch.tensor(X_test.drop(columns=['ë³‘ëª…']).astype(np.float32).values)

y_train_tensor = torch.tensor(y_train.astype(np.float32).values)
y_val_tensor = torch.tensor(y_val.astype(np.float32).values)
y_test_tensor = torch.tensor(y_test.astype(np.float32).values)

# TensorDataset: (ë³‘ëª…ID, ì…ë ¥íŠ¹ì„±, íƒ€ê²Ÿ)
train_dataset = TensorDataset(disease_train, X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(disease_val, X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(disease_test, X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=128)
test_loader = DataLoader(test_dataset, batch_size=128)

################################################################################################
####################################### AI Design and Training #################################
################################################################################################

class ComplexMLPWithEmbedding(nn.Module):
    def __init__(self, num_diseases, embedding_dim, input_dim, hidden_dims, bottleneck_dim, output_dim):
        super(ComplexMLPWithEmbedding, self).__init__()

        self.embedding = nn.Embedding(num_diseases, embedding_dim)

        layers = []
        total_input_dim = embedding_dim + input_dim  # ë³‘ëª… ì„ë² ë”© + ë‚˜ë¨¸ì§€ ì…ë ¥
        prev_dim = total_input_dim
        for h in hidden_dims:
            layers.append(nn.Linear(prev_dim, h))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(h))
            layers.append(nn.Dropout(0.3))
            prev_dim = h
        layers.append(nn.Linear(prev_dim, bottleneck_dim))
        layers.append(nn.Sigmoid())
        # layers.append(nn.Softplus())
        layers.append(nn.Linear(bottleneck_dim, output_dim))
        self.network = nn.Sequential(*layers)

    def forward(self, disease_ids, features):
        embedded = self.embedding(disease_ids)  # (batch, embedding_dim)
        x = torch.cat((embedded, features), dim=1)
        return self.network(x)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
num_diseases = X['ë³‘ëª…'].nunique()
embedding_dim = 16
input_dim = X_train_tensor.shape[1]  # ë³‘ëª… ì œì™¸ ë‚˜ë¨¸ì§€ ì…ë ¥
hidden_dims = [256, 128, 128, 64, 64]
bottleneck_dim = 1
output_dim = 4

model = ComplexMLPWithEmbedding(num_diseases, embedding_dim, input_dim, hidden_dims, bottleneck_dim, output_dim)

# Loss ë° Optimizer ì •ì˜
#criterion = nn.MSELoss()
#criterion = nn.L1Loss()
criterion = nn.SmoothL1Loss()  # Huber
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# í•™ìŠµ ê¸°ë¡ìš© ë³€ìˆ˜
train_losses = []
val_losses = []
best_val_loss = float('inf')
patience = 30
counter = 0

for epoch in range(1000):
    model.train()
    epoch_train_loss = 0.0
    for disease_id, xb, yb in train_loader:
        disease_id, xb, yb = disease_id.to(device), xb.to(device), yb.to(device)
        optimizer.zero_grad()
        preds = model(disease_id, xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        epoch_train_loss += loss.item() * xb.size(0)
    epoch_train_loss /= len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # validation
    model.eval()
    epoch_val_loss = 0.0
    with torch.no_grad():
        for disease_id, xb, yb in val_loader:
            disease_id, xb, yb = disease_id.to(device), xb.to(device), yb.to(device)
            preds = model(disease_id, xb)
            loss = criterion(preds, yb)
            epoch_val_loss += loss.item() * xb.size(0)
    epoch_val_loss /= len(val_loader.dataset)
    val_losses.append(epoch_val_loss)

    # ReduceLROnPlateau í™•ì¸
    prev_lr = optimizer.param_groups[0]['lr']
    scheduler.step(epoch_val_loss)
    new_lr = optimizer.param_groups[0]['lr']
    if new_lr < prev_lr:
        print(f"ğŸ“‰ Epoch {epoch + 1:03d} | LR reduced from {prev_lr:.8f} to {new_lr:.8f}")

    print(f"Epoch {epoch+1:03d} | Train Loss: {epoch_train_loss:.8f} | Val Loss: {epoch_val_loss:.8f}")

    # Early stopping
    if epoch_val_loss < best_val_loss:
        best_val_loss = epoch_val_loss
        counter = 0
        torch.save(model.state_dict(), "best_model.pt")
    else:
        counter += 1
        if counter >= patience:
            print("ğŸ›‘ Early stopping triggered.")
            break

################################################################################################
####################################### AI Test ################################################
################################################################################################

model.load_state_dict(torch.load("best_model.pt"))
model.eval()
preds_list = []
targets_list = []

with torch.no_grad():
    for disease_id, xb, yb in test_loader:
        disease_id, xb = disease_id.to(device), xb.to(device)
        preds = model(disease_id, xb).cpu().numpy()
        preds_list.append(preds)
        targets_list.append(yb.numpy())

preds_all = np.vstack(preds_list)
targets_all = np.vstack(targets_list)

preds_original = scaler.inverse_transform(preds_all)
targets_original = scaler.inverse_transform(targets_all)

################################################################################################
####################################### Save ###################################################
################################################################################################
file_name = 'MLP'

model_save_dir = f"./Care_Duration_Prediction_Model/Modeling/{file_name}/Model"
result_save_dir = f"./Care_Duration_Prediction_Model/Modeling/{file_name}/Result"
os.makedirs(model_save_dir, exist_ok=True)
os.makedirs(result_save_dir, exist_ok=True)

model_path = os.path.join(model_save_dir, "best_model.pt")
result_path = os.path.join(result_save_dir, "predict_result.png")

torch.save(model.state_dict(), model_path)

# ì‹œê°í™”: ì˜ˆì¸¡ vs ì‹¤ì œ (ì‚°ì ë„)
plt.figure(figsize=(12, 8))
for i, label in enumerate(['ì„±ë³„_ìš”ì–‘ì¼', 'ìˆ˜ìˆ ì—¬ë¶€_ìš”ì–‘ì¼', 'ì—°ë ¹ëŒ€_ìš”ì–‘ì¼', 'ì§€ì—­ë³¸ë¶€_ìš”ì–‘ì¼']):
    plt.subplot(2, 2, i+1)
    plt.plot(targets_original[:, i], color = 'black')
    plt.plot(preds_original[:, i],color = 'red' )
    plt.ylabel(f"value {label}")
    plt.title(label)
plt.tight_layout()
plt.savefig(result_path)
plt.close()

print(f"\nğŸ“Š Evaluation Metrics (ë‹¨ìœ„: ì›ë˜ ìŠ¤ì¼€ì¼)\n{'=' * 40}")

for i, label in enumerate(['ì„±ë³„_ìš”ì–‘ì¼', 'ìˆ˜ìˆ ì—¬ë¶€_ìš”ì–‘ì¼', 'ì—°ë ¹ëŒ€_ìš”ì–‘ì¼', 'ì§€ì—­ë³¸ë¶€_ìš”ì–‘ì¼']):
    y_true = targets_original[:, i]
    y_pred = preds_original[:, i]

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # %

    print(f"ğŸ“Œ [{label}]")
    print(f"  MSE  : {mse:.4f}")
    print(f"  RMSE : {rmse:.4f}")
    print(f"  MAE  : {mae:.4f}")
    print(f"  MAPE : {mape:.2f}%\n")


################################################################################################
####################################### Final task #############################################
################################################################################################

# hook ì¶œë ¥ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
bottleneck_outputs = []

# hook í•¨ìˆ˜ ì •ì˜
def save_bottleneck_output(module, input, output):
    bottleneck_outputs.append(output.detach().cpu().numpy())

# bottleneck ReLU ë ˆì´ì–´ì— hook ë“±ë¡
bottleneck_layer = model.network[21]
hook_handle = bottleneck_layer.register_forward_hook(save_bottleneck_output)

# ì „ì²´ ë°ì´í„°ì…‹ (train + val + test)
full_dataset = ConcatDataset([train_dataset, val_dataset, test_dataset])
full_loader = DataLoader(full_dataset, batch_size=128, shuffle=False)

# ì¶”ë¡  (forwardë§Œ í•˜ë©´ hookì´ ìë™ ì‘ë™)
with torch.no_grad():
    for disease_id, xb, _ in full_loader:
        disease_id, xb = disease_id.to(device), xb.to(device)
        _ = model(disease_id, xb)  # predsëŠ” í•„ìš” ì—†ìŒ, hookì´ bottleneck ì €ì¥í•¨

# ë³‘í•©
bottleneck_all = np.vstack(bottleneck_outputs)  # shape: (ì „ì²´ ìƒ˜í”Œ ìˆ˜, 1)

# hook í•´ì œ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
hook_handle.remove()

################################################################################################
####################################### Save ###################################################
################################################################################################

# ë³‘ëª… ë³µì›
inverse_diseases = le.inverse_transform(X['ë³‘ëª…'])

# ì„±ë³„, ì—°ë ¹ëŒ€, ì§€ì—­ë³¸ë¶€ ë³µì›
def reverse_one_hot(df, prefix):
    cols = [col for col in df.columns if col.startswith(prefix + '_')]
    return df[cols].idxmax(axis=1).str.replace(f"{prefix}_", "")
sex = reverse_one_hot(X, 'ì„±ë³„')
age = reverse_one_hot(X, 'ì—°ë ¹ëŒ€')
region = reverse_one_hot(X, 'ì§€ì—­ë³¸ë¶€')

# ì •ë‹µ í‰ê· ê°’ ê³„ì‚°
y_original = scaler.inverse_transform(y)
y_mean = y_original.mean(axis=1)

# bottleneck ê°’ ë³´ì •
scaler_bottleneck = MinMaxScaler()
bottleneck_norm = scaler_bottleneck.fit_transform(bottleneck_all)
#bottleneck_scaled = np.sqrt(bottleneck_norm)
#bottleneck_scaled = np.log1p(bottleneck_norm)
bottleneck_rescaled = bottleneck_norm.flatten() * y_mean.max()

# ë³‘í•©
result_df = pd.DataFrame({
    'ë³‘ëª…': inverse_diseases,
    'ì„±ë³„': sex,
    'ì—°ë ¹ëŒ€': age,
    'ì§€ì—­ë³¸ë¶€': region,
    'í‰ê· _ìš”ì–‘ì¼': y_mean,
    'bottleneck_output': bottleneck_rescaled.flatten()
})
# ì €ì¥
final_save_dir = f"./Care_Duration_Prediction_Model/Modeling/{file_name}/Final"
os.makedirs(final_save_dir, exist_ok=True)
final_path = os.path.join(final_save_dir, "bottleneck_output.csv")
result_df.to_csv(final_path, index=False, encoding='utf-8-sig')

################################################################################################
####################################### Final Result Analysis ##################################
################################################################################################

# 1. ì‚°ì ë„ ì‹œê°í™”
plt.figure(figsize=(8, 6))
sns.scatterplot(x='í‰ê· _ìš”ì–‘ì¼', y='bottleneck_output', data=result_df, alpha=0.5)
plt.title("ğŸ“ˆ í‰ê·  ìš”ì–‘ì¼ vs Bottleneck ì¶œë ¥")
plt.xlabel("í‰ê·  ìš”ì–‘ì¼")
plt.ylabel("Bottleneck ì¶œë ¥")
plt.grid(True)
plt.tight_layout()
plt.savefig(final_save_dir + "/scatter_í‰ê· ìš”ì–‘ì¼_vs_bottleneck.png")
plt.close()

# 2. íˆìŠ¤í† ê·¸ë¨ (ë¶„í¬ ë³´ê¸°)
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
sns.histplot(result_df['í‰ê· _ìš”ì–‘ì¼'], kde=True, color="blue")
plt.title("ğŸ“Š í‰ê·  ìš”ì–‘ì¼ ë¶„í¬")

plt.subplot(1, 2, 2)
sns.histplot(result_df['bottleneck_output'], kde=True, color="red")
plt.title("ğŸ“Š Bottleneck ì¶œë ¥ ë¶„í¬")

plt.tight_layout()
plt.savefig(final_save_dir + "/histograms_í‰ê· ìš”ì–‘ì¼_bottleneck.png")
plt.close()

# 3. ìƒê´€ê³„ìˆ˜ (Pearson / Spearman)
pearson_corr, _ = pearsonr(result_df['í‰ê· _ìš”ì–‘ì¼'], result_df['bottleneck_output'])
spearman_corr, _ = spearmanr(result_df['í‰ê· _ìš”ì–‘ì¼'], result_df['bottleneck_output'])

print(f'pearson_corr: {pearson_corr}, spearman_corr: {spearman_corr}')
