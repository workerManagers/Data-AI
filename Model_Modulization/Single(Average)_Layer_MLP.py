import itertools
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

df = pd.read_csv('/Users/hwangtaeeon/Documents/GitHub/Data-AI/Care_Duration_Prediction_Model/Data_Preprocessing/Regular_Expression/final_data.csv')

# ê³ ìœ  ë³‘ëª… ë¦¬ìŠ¤íŠ¸
diseases = df['ë³‘ëª…'].tolist()

# ê° íŠ¹ì„±ë³„ ë²”ì£¼ ì •ì˜
genders = ['ì„±ë³„_ë‚¨ì', 'ì„±ë³„_ì—¬ì']
surgery_status = ['ìˆ˜ìˆ ì—¬ë¶€_ì•„ë‹ˆì˜¤', 'ìˆ˜ìˆ ì—¬ë¶€_ì˜ˆ']
age_groups = ['ì—°ë ¹ëŒ€_30ì„¸ë¯¸ë§Œ', 'ì—°ë ¹ëŒ€_30-39ì„¸', 'ì—°ë ¹ëŒ€_40-49ì„¸', 'ì—°ë ¹ëŒ€_50-59ì„¸', 'ì—°ë ¹ëŒ€_60ì„¸ì´ìƒ']
regions = ['ì§€ì—­ë³¸ë¶€_ì„œìš¸ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ë¶€ì‚°ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ëŒ€êµ¬ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ê²½ì¸ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ê´‘ì£¼ì§€ì—­', 'ì§€ì—­ë³¸ë¶€_ëŒ€ì „ì§€ì—­']

# ëª¨ë“  ì¡°í•© ìƒì„± (ë³‘ëª… í¬í•¨ ì´ 5ê°œì˜ ì…ë ¥)
input_combinations = list(itertools.product(diseases, genders, surgery_status, age_groups, regions))

# ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
processed_data = []

# ì¡°í•©ë§ˆë‹¤ í‰ê·  ì˜ˆì¸¡ê°’ ê³„ì‚°
for disease, gender, surgery, age, region in input_combinations:
    row = df[df['ë³‘ëª…'] == disease]
    if row.empty:
        continue
    values = row[[gender, surgery, age, region]].values[0]
    if pd.isnull(values).any():
        continue  # ê²°ì¸¡ê°’ í¬í•¨ ì‹œ ì œì™¸
    mean_value = values.mean()
    processed_data.append([disease, gender, surgery, age, region, mean_value])

# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
processed_df = pd.DataFrame(processed_data, columns=['ë³‘ëª…', 'ì„±ë³„', 'ìˆ˜ìˆ ì—¬ë¶€', 'ì—°ë ¹ëŒ€', 'ì§€ì—­ë³¸ë¶€', 'ì˜ˆì¸¡ê°’'])

processed_df.to_csv('processed_prediction_data.csv', index=False)

# ğŸ“Œ Label Encoding
encoders = {}
X = pd.DataFrame()
for col in ['ë³‘ëª…', 'ì„±ë³„', 'ìˆ˜ìˆ ì—¬ë¶€', 'ì—°ë ¹ëŒ€', 'ì§€ì—­ë³¸ë¶€']:
    enc = LabelEncoder()
    X[col] = enc.fit_transform(processed_df[col])
    encoders[col] = enc

y = processed_df['ì˜ˆì¸¡ê°’'].values

# ğŸ“Œ Stratified split
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=X['ë³‘ëª…'], random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, stratify=X_trainval['ë³‘ëª…'], random_state=42)

# ğŸ“Œ TensorDataset
def to_tensor(x, y):
    return TensorDataset(torch.tensor(x.values, dtype=torch.long), torch.tensor(y, dtype=torch.float32))

train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=64, shuffle=True)
val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=64)
test_loader = DataLoader(to_tensor(X_test, y_test), batch_size=64)

# ğŸ“Œ MLP Model
class MLPRegressor(nn.Module):
    def __init__(self, input_dims, embed_dim=8):
        super().__init__()
        self.embeds = nn.ModuleList([
            nn.Embedding(dim, embed_dim) for dim in input_dims
        ])
        self.fc1 = nn.Linear(embed_dim * len(input_dims), 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 64)
        self.out = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.cat([emb(x[:, i]) for i, emb in enumerate(self.embeds)], dim=1)
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.fc2(x))
        return self.out(x).squeeze()


# ğŸ“Œ Model Setup
input_dims = [X[col].nunique() for col in X.columns]
model = MLPRegressor(input_dims)

optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=True)
criterion = nn.MSELoss()

# ğŸ“Œ Training Loop with Early Stopping
best_val_loss = float('inf')
early_stop_counter = 0
train_losses, val_losses = [], []

for epoch in range(1000):
    model.train()
    train_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        pred = model(xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * len(xb)

    train_loss /= len(train_loader.dataset)
    train_losses.append(train_loss)

    model.eval()
    val_loss = 0
    with torch.no_grad():
        for xb, yb in val_loader:
            pred = model(xb)
            loss = criterion(pred, yb)
            val_loss += loss.item() * len(xb)
    val_loss /= len(val_loader.dataset)
    val_losses.append(val_loss)

    scheduler.step(val_loss)
    print(f"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}")
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= 10:
            print("âœ… Early stopping!")
            break

# ğŸ“ˆ Loss Curve
plt.plot(train_losses, label='Train')
plt.plot(val_losses, label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')
plt.show()

# ğŸ“Œ Test Set Evaluation & Visualization
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

predictions = []
actuals = []

with torch.no_grad():
    for xb, yb in test_loader:
        pred = model(xb)
        predictions.extend(pred.tolist())
        actuals.extend(yb.tolist())

# ğŸ“ˆ ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ ì‹œê°í™”
plt.figure(figsize=(6, 5))
plt.plot(actuals, label='Actual', alpha=0.7)
plt.plot(predictions, label='Predicted', alpha=0.7)
plt.title('Predicted vs Actual (Test Set)')
plt.xlabel('Sample Index')
plt.ylabel('ì˜ˆì¸¡ê°’')
plt.legend()
plt.tight_layout()
plt.show()