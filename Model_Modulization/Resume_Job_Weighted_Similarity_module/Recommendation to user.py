# -*- coding: utf-8 -*-
import json
import torch
import re
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
model = SentenceTransformer('jhgan/ko-sbert-sts')
tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sbert-sts')

# âœ… ë¬¸ì¥ + í† í° ìˆ˜ ê¸°ì¤€ ë¸”ë¡ ìª¼ê°œê¸°
def split_into_token_safe_chunks(text, token_limit=512, char_hint=200):
    text = text.replace('\n', ' ')
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())

    results = []
    current_chunk = ""
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue

        temp_chunk = (current_chunk + " " + sent).strip()
        token_len = len(tokenizer.encode(temp_chunk, add_special_tokens=True))

        if token_len <= token_limit and len(temp_chunk) <= char_hint * 2:
            current_chunk = temp_chunk
        else:
            if current_chunk:
                results.append(current_chunk.strip())
            current_chunk = sent

    if current_chunk:
        results.append(current_chunk.strip())

    return results

# âœ… ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
def weighted_average_similarity(resume, jobPost_description):
    resume_chunks = split_into_token_safe_chunks(resume)
    job_chunks = split_into_token_safe_chunks(jobPost_description)

    sims = []
    for res in resume_chunks:
        res_vec = model.encode(res, convert_to_tensor=True)
        for job in job_chunks:
            job_vec = model.encode(job, convert_to_tensor=True)
            sim = util.cos_sim(res_vec, job_vec).item()
            weight = sim ** 2
            sims.append((sim, weight))

    if not sims:
        return 0.0

    weighted_sum = sum(sim * weight for sim, weight in sims)
    total_weight = sum(weight for _, weight in sims)
    return weighted_sum / total_weight if total_weight != 0 else 0.0


# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("\nğŸ” ìì†Œì„œ vs ì±„ìš©ê³µê³  ìœ ì‚¬ë„ ë¹„êµ ì‹œì‘")

    # âœ… ìì†Œì„œ ì§ì ‘ ì…ë ¥
    self_intro = """ì €ëŠ” ê¾¸ì¤€í•œ ë…¸ë ¥ê³¼ í˜‘ì—…ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ê°€ ë˜ê³ ì í•©ë‹ˆë‹¤. ëŒ€í•™êµ ì¬í•™ ì¤‘ ë‹¤ì–‘í•œ íŒ€ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì‹¤ì „ ê°ê°ì„ ìµí˜”ìœ¼ë©°, íŠ¹íˆ ìº¡ìŠ¤í†¤ë””ìì¸ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°ë¥¼ ê°œë°œí•˜ì—¬ 3ê°œì›”ê°„ì˜ í˜‘ì—…ì„ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬í–ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ TensorFlowì™€ Pythonì„ í™œìš©í•œ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ê°œì„  ê²½í—˜ì„ ìŒ“ì•˜ìŠµë‹ˆë‹¤.\në˜í•œ êµë‚´ ì•Œê³ ë¦¬ì¦˜ ìŠ¤í„°ë””ë¥¼ í†µí•´ ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”í•˜ì˜€ìœ¼ë©°, C++ê³¼ Javaë¥¼ í™œìš©í•´ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì™¸ì—ë„ í´ë¼ìš°ë“œ ê¸°ë°˜ í™˜ê²½ì—ì„œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•œ ê²½í—˜ì´ ìˆì–´, AWS í™˜ê²½ì—ì„œ EC2, RDS ë“±ì„ ì‚¬ìš©í•´ ë°±ì—”ë“œ ì„œë²„ë¥¼ êµ¬ì¶•í•œ ë°” ìˆìŠµë‹ˆë‹¤.\nì €ëŠ” í•­ìƒ íŒ€ì˜ ëª©í‘œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ìƒê°í•˜ë©° ì†Œí†µí•˜ê³ , ìƒˆë¡œìš´ ê¸°ìˆ ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•Šê³  ë°°ìš°ëŠ” ê²ƒì„ ì¦ê¹ë‹ˆë‹¤. ì‚¼ì„±ì „ìì˜ ê¸€ë¡œë²Œ í™˜ê²½ì—ì„œ ëŠì„ì—†ì´ ì„±ì¥í•˜ë©°, ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ê°œë°œí•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."""

    # âœ… JSON íŒŒì¼ì—ì„œ ì±„ìš©ê³µê³  ë¡œë”© (id í¬í•¨)
    try:
        json_path = "/Users/hwangtaeeon/Documents/GitHub/Data-AI/Model_Modulization/Resume_Job_Weighted_Similarity_module/post_dataset.json"
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        exit(1)

    jobPost_description_list = [
        {"jobPost_id": item.get("jobPost_id", ), "text": item.get("jobPost_description", "")}
        for idx, item in enumerate(data)
        if item.get("jobPost_description")
    ]

    results = []

    for job in jobPost_description_list:
        jobPost_id = job["jobPost_id"]
        job_text = job["text"]
        sim = weighted_average_similarity(self_intro, job_text)

        job_text = job_text.strip().replace('\n', ' ')
        self_intro = self_intro.strip().replace('\n', ' ')

        results.append({
            "jobPost_id": jobPost_id,
            "similarity": sim,
        })

    # ğŸ”½ ìœ ì‚¬ë„ ìˆœ ì •ë ¬
    sorted_results = sorted(results, key=lambda x: x["similarity"], reverse=True)

    # âœ… JSON í˜•ì‹ìœ¼ë¡œ ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ
    top_json_results = [
        {"jobPost_id": item["jobPost_id"], "similarity": round(item["similarity"], 4)}
        for item in sorted_results
    ]

    # âœ… JSON ì €ì¥
    output_path = "top_match_results.json"
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(top_json_results, f, ensure_ascii=False, indent=4)
        print(f"\nâœ… ê²°ê³¼ê°€ '{output_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
