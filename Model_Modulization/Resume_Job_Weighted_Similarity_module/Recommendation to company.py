# -*- coding: utf-8 -*-
import json
import torch
import re
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
model = SentenceTransformer('jhgan/ko-sbert-sts')
tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sbert-sts')

# âœ… ë¬¸ì¥ + í† í° ìˆ˜ ê¸°ì¤€ ë¸”ë¡ ìª¼ê°œê¸°
def split_into_token_safe_chunks(text, token_limit=512, char_hint=200):
    text = text.replace('\n', ' ')
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())

    results = []
    current_chunk = ""
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue

        temp_chunk = (current_chunk + " " + sent).strip()
        token_len = len(tokenizer.encode(temp_chunk, add_special_tokens=True))

        if token_len <= token_limit and len(temp_chunk) <= char_hint * 2:
            current_chunk = temp_chunk
        else:
            if current_chunk:
                results.append(current_chunk.strip())
            current_chunk = sent

    if current_chunk:
        results.append(current_chunk.strip())

    return results

# âœ… ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
def weighted_average_similarity(resume, resume_description):
    resume_chunks = split_into_token_safe_chunks(resume)
    job_chunks = split_into_token_safe_chunks(resume_description)

    sims = []
    for res in resume_chunks:
        res_vec = model.encode(res, convert_to_tensor=True)
        for job in job_chunks:
            job_vec = model.encode(job, convert_to_tensor=True)
            sim = util.cos_sim(res_vec, job_vec).item()
            weight = sim ** 2
            sims.append((sim, weight))

    if not sims:
        return 0.0

    weighted_sum = sum(sim * weight for sim, weight in sims)
    total_weight = sum(weight for _, weight in sims)
    return weighted_sum / total_weight if total_weight != 0 else 0.0

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("\nğŸ” ì±„ìš©ê³µê³  vs ìì†Œì„œ ìœ ì‚¬ë„ ë¹„êµ ì‹œì‘")

    # âœ… ì±„ìš©ê³µê³  ì§ì ‘ ì…ë ¥ 
    jobpost_input = """[ì‚¼ì„±ì „ì - DXë¶€ë¬¸ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ ì‹ ì… ì±„ìš©]\nì‚¼ì„±ì „ìëŠ” ê¸€ë¡œë²Œ IT ì‹œì¥ì„ ì„ ë„í•  ì¸ì¬ë¥¼ ëª¨ì§‘í•©ë‹ˆë‹¤. ë‹¹ì‚¬ëŠ” ì°¨ì„¸ëŒ€ ì†Œí”„íŠ¸ì›¨ì–´ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í˜ì‹ ì ì¸ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ê³ ê°ì˜ ì‚¶ì„ ë”ìš± í’ìš”ë¡­ê²Œ í•˜ê³ ì í•©ë‹ˆë‹¤. DXë¶€ë¬¸ì€ ìŠ¤ë§ˆíŠ¸í°, ê°€ì „ì œí’ˆ, í—¬ìŠ¤ì¼€ì–´ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë° ì„œë¹„ìŠ¤ í”Œë«í¼ì„ êµ¬ì¶•í•˜ê³  ìˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì¸ì¬ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.\n\n[ì£¼ìš” ì—…ë¬´]\n- ì„ë² ë””ë“œ ì‹œìŠ¤í…œ ë° ì–´í”Œë¦¬ì¼€ì´ì…˜ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ\n- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë° í´ë¼ìš°ë“œ ê¸°ë°˜ ì„œë¹„ìŠ¤ ê°œë°œ\n- AI/ML ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ ê¸°ëŠ¥ ê³ ë„í™”\n- ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ìš´ì˜ì„ ìœ„í•œ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° ê°œë°œ\n\n[ìê²© ìš”ê±´]\n- ì»´í“¨í„°ê³µí•™, ì „ìê³µí•™ ë“± ê´€ë ¨ ì „ê³µ í•™ì‚¬ ì´ìƒ\n- Python, C/C++, Java ë“± í”„ë¡œê·¸ë˜ë° ì–¸ì–´ í™œìš© ëŠ¥ë ¥\n- ìë£Œêµ¬ì¡°, ì•Œê³ ë¦¬ì¦˜, OS, ë„¤íŠ¸ì›Œí¬ ë“± ê¸°ë³¸ CS ì§€ì‹ ë³´ìœ \n- í˜‘ì—… ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ ìš°ìˆ˜ì\n\n[ìš°ëŒ€ ì‚¬í•­]\n- ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ ì°¸ì—¬ ê²½í—˜\n- AI/ML, ë¹…ë°ì´í„° ê´€ë ¨ í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²½í—˜\n- ì˜ì–´ ë° ê¸°íƒ€ ì™¸êµ­ì–´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥\n\ní˜ì‹ ì„ ì´ëŒì–´ê°ˆ ë„ì „ì ì¸ ì¸ì¬ë“¤ì˜ ë§ì€ ì§€ì› ë°”ëë‹ˆë‹¤."""

    # âœ… JSON íŒŒì¼ì—ì„œ ìì†Œì„œ ë¡œë”© (id í¬í•¨)
    try:
        json_path = "/Users/hwangtaeeon/Documents/GitHub/Data-AI/Model_Modulization/Resume_Job_Weighted_Similarity_module/resume_dataset.json"
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        exit(1)

    resume_description_list = [
        {"resume_id": item.get("resume_id", ), "text": item.get("resume_description", "")}
        for idx, item in enumerate(data)
        if item.get("resume_description")
    ]

    results = []

    for job in resume_description_list:
        resume_id = job["resume_id"]
        job_text = job["text"]
        sim = weighted_average_similarity(jobpost_input, job_text)

        job_text = job_text.strip().replace('\n', ' ')
        jobpost_input = jobpost_input.strip().replace('\n', ' ')

        results.append({
            "resume_id": resume_id,
            "similarity": sim,
        })

    # ğŸ”½ ìœ ì‚¬ë„ ìˆœ ì •ë ¬
    sorted_results = sorted(results, key=lambda x: x["similarity"], reverse=True)

    # âœ… JSON í˜•ì‹ìœ¼ë¡œ ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ
    top_json_results = [
        {"resume_id": item["resume_id"], "similarity": round(item["similarity"], 4)}
        for item in sorted_results
    ]

    # âœ… JSON ì €ì¥
    output_path = "top_resume_results.json"
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(top_json_results, f, ensure_ascii=False, indent=4)
        print(f"\nâœ… ê²°ê³¼ê°€ '{output_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
