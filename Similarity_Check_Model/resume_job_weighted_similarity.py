# -*- coding: utf-8 -*-
import torch
import re
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
model = SentenceTransformer('jhgan/ko-sbert-sts')
tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sbert-sts')

# âœ… ë¬¸ì¥ ê¸°ì¤€ + í† í° ìˆ˜ ê¸°ì¤€ ë¸”ë¡ ìª¼ê°œê¸°
def split_into_token_safe_chunks(text, token_limit=512, char_hint=200):
    text = text.replace('\n', ' ')
    
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    results = []

    current_chunk = ""
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue

        temp_chunk = (current_chunk + " " + sent).strip()
        token_len = len(tokenizer.encode(temp_chunk, add_special_tokens=True))

        if token_len <= token_limit and len(temp_chunk) <= char_hint * 2:
            current_chunk = temp_chunk
        else:
            if current_chunk:
                results.append(current_chunk.strip())
            current_chunk = sent  # ìƒˆ ë¬¸ì¥ ì‹œì‘

    if current_chunk:
        results.append(current_chunk.strip())

    return results

# âœ… í‰ê·  ì„ë² ë”© ë°©ì‹
def encode_and_average(text):
    chunks = split_into_token_safe_chunks(text)
    embeddings = [model.encode(chunk, convert_to_tensor=True) for chunk in chunks]
    return torch.stack(embeddings).mean(dim=0)

# âœ… ê°€ì¤‘ í‰ê·  ë°©ì‹
def weighted_average_similarity(resume, job_posting):
    resume_chunks = split_into_token_safe_chunks(resume)
    job_chunks = split_into_token_safe_chunks(job_posting)

    sims = []
    for res in resume_chunks:
        res_vec = model.encode(res, convert_to_tensor=True)
        for job in job_chunks:
            job_vec = model.encode(job, convert_to_tensor=True)
            sim = util.cos_sim(res_vec, job_vec).item()
            sims.append((sim, sim**2))

    if not sims:
        return 0.0

    weighted_sum = sum(sim * weight for sim, weight in sims)
    total_weight = sum(weight for _, weight in sims)
    return weighted_sum / total_weight if total_weight != 0 else 0.0

# âœ… ë””ë²„ê¹…ìš©: ë¸”ë¡ë³„ ìœ ì‚¬ë„ í™•ì¸ìš©
def weighted_average_similarity_debug(resume, job_posting):
    resume_chunks = split_into_token_safe_chunks(resume)
    job_chunks = split_into_token_safe_chunks(job_posting)

    debug_info = []  # (resume_chunk, job_chunk, sim, weight)
    sims = []

    for res in resume_chunks:
        res_vec = model.encode(res, convert_to_tensor=True)
        for job in job_chunks:
            job_vec = model.encode(job, convert_to_tensor=True)
            sim = util.cos_sim(res_vec, job_vec).item()
            weight = sim ** 2
            sims.append((sim, weight))
            debug_info.append((res, job, sim, weight))

    if not sims:
        return 0.0, []

    weighted_sum = sum(sim * weight for sim, weight in sims)
    total_weight = sum(weight for _, weight in sims)
    weighted_avg = weighted_sum / total_weight if total_weight != 0 else 0.0

    return weighted_avg, debug_info

# âœ… ìœ ì‚¬ë„ ê³„ì‚° í†µí•© í•¨ìˆ˜
def calculate_similarity(resume, job_posting, mode='mean'):
    if mode == 'mean':
        resume_vec = encode_and_average(resume)
        job_vec = encode_and_average(job_posting)
        return util.cos_sim(resume_vec, job_vec).item()
    elif mode == 'weighted':
        return weighted_average_similarity(resume, job_posting)
    else:
        raise ValueError("modeëŠ” 'mean' ë˜ëŠ” 'weighted'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# âœ… ì˜ˆì‹œ ì…ë ¥
resume = """
[í´ë¦°ì‹œí‹° - ê±´ë¬¼ ì²­ì†Œ ë° í™˜ê²½ë¯¸í™” ì¸ë ¥ ì±„ìš©] í´ë¦°ì‹œí‹°ëŠ” ì˜¤í”¼ìŠ¤, ìƒê°€, í•™êµ ë“± ë‹¤ì–‘í•œ ì‹œì„¤ì˜ ìœ„ìƒê´€ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ í™˜ê²½ë¯¸í™” ê¸°ì—…ì…ë‹ˆë‹¤. ë³¸ ì§ë¬´ëŠ” ì •í•´ì§„ êµ¬ì—­ì˜ ì‹¤ë‚´Â·ì™¸ ì²­ì†Œ ë° íê¸°ë¬¼ ìˆ˜ê±°, ì‹œì„¤ ë¯¸í™” ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©°, ë°˜ë³µì„±ê³¼ ì²´ë ¥ ì†Œëª¨ê°€ ë†’ì€ ì§ë¬´ì…ë‹ˆë‹¤. [ì£¼ìš” ì—…ë¬´] - ì‚¬ë¬´ì‹¤, ê³„ë‹¨, ë³µë„, í™”ì¥ì‹¤ ë“± ì§€ì • êµ¬ì—­ ì²­ì†Œ - ë¶„ë¦¬ìˆ˜ê±°, ì“°ë ˆê¸° ìˆ˜ê±° ë° íê¸°ë¬¼ ë°°ì¶œ - ì²­ì†Œë„êµ¬ ë° ì¥ë¹„ ê´€ë¦¬, ì²­ì†Œìš©í’ˆ ì¬ê³  ì ê²€ - ì •ê¸° ì†Œë… ë° ë°©ì—­ ì‘ì—… ë³´ì¡° (êµìœ¡ ì´ìˆ˜ í›„) [ìê²© ìš”ê±´] - í•™ë ¥ ë¬´ê´€, ì—°ë ¹ ë¬´ê´€ (ê³ ë ¹ì í™˜ì˜) - ì²­ê²°Â·ìœ„ìƒ ì˜ì‹ ë³´ìœ ì - ë°˜ë³µ ì‘ì—… ë° ì‹¤ë‚´ì™¸ ì´ë™ ì—…ë¬´ ìˆ˜í–‰ ê°€ëŠ¥ì - ê¸°ë³¸ì ì¸ ì¥ë¹„ ì‚¬ìš© ë° ì—…ë¬´ ë³´ê³  ê°€ëŠ¥ì [ìš°ëŒ€ ì‚¬í•­] - ì²­ì†ŒÂ·ë¯¸í™” ê²½ë ¥ì - ê°ì—¼ë³‘ ì˜ˆë°©êµìœ¡, ìœ„ìƒì•ˆì „êµìœ¡ ì´ìˆ˜ì - ì¥ì• ì¸, ê³ ë ¹ì, ê²½ë ¥ë‹¨ì ˆì ìš°ëŒ€ ì‘ê³  ë¬µë¬µí•œ ì¼ì´ì§€ë§Œ, ëª¨ë‘ê°€ ê¹¨ë—í•˜ê²Œ ì‚´ì•„ê°ˆ ìˆ˜ ìˆëŠ” ê¸°ë³¸ì„ ë§Œë“¤ì–´ê°ˆ ë¶„ë“¤ì˜ ë§ì€ ì§€ì› ë°”ëë‹ˆë‹¤.
"""

job_posting = """
ì €ëŠ” ëˆ„ê°€ ë³´ì§€ ì•Šì•„ë„ ì„±ì‹¤íˆ ìê¸° êµ¬ì—­ì„ ì±…ì„ì§€ëŠ” ì²­ì†Œ ì¸ë ¥ì´ ë˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ê·¸ë™ì•ˆ í•™êµ ì²­ì†Œ ë³´ì¡°, ê³µë™ì£¼íƒ ê³„ë‹¨ ë¯¸í™” ë“±ì˜ ì¼ì„ í•˜ë©° ë¨¼ì§€ í•˜ë‚˜, ì–¼ë£© í•˜ë‚˜ê¹Œì§€ ì‹ ê²½ ì“°ëŠ” ìŠµê´€ì„ ë“¤ì˜€ê³ , ì •í•´ì§„ ë£¨í‹´ì— ë”°ë¼ ì² ì €íˆ ì²­ì†Œí•˜ë©´ì„œ â€˜ìœ„ìƒâ€™ì˜ ì¤‘ìš”ì„±ì„ ì²´ê°í–ˆìŠµë‹ˆë‹¤. ì‘ì—… ì¤‘ì—ëŠ” ë¬´ë¦ê³¼ í—ˆë¦¬ë¥¼ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì¼ì´ ë§ê¸°ì— ë¬´ë¦¬í•˜ì§€ ì•Šê³  ìì„¸ë¥¼ ë°”ë¥´ê²Œ ìœ ì§€í•˜ëŠ” ë²•ì„ ìµí˜”ê³ , ì¥ê°‘ê³¼ ë§ˆìŠ¤í¬ë¥¼ í•­ìƒ ì°©ìš©í•˜ë©° ê°ì—¼ì„± ì˜¤ì—¼ë¬¼ë„ ì•ˆì „í•˜ê²Œ ë‹¤ë£¨ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. ë§¤ì¼ê°™ì´ ê°™ì€ ì¼ì„ ë°˜ë³µí•´ë„, ì €ëŠ” ê·¸ ì¼ì´ ëˆ„êµ°ê°€ì—ê²ŒëŠ” ì¾Œì í•œ í•˜ë£¨ì˜ ì‹œì‘ì´ ëœë‹¤ê³  ë¯¿ê³  ìˆìŠµë‹ˆë‹¤. í´ë¦°ì‹œí‹°ì—ì„œ ë¬µë¬µíˆ, ê·¸ëŸ¬ë‚˜ ì •í™•í•˜ê²Œ ìê¸° ìë¦¬ë¥¼ ì§€í‚¤ëŠ” í™˜ê²½ë¯¸í™”ì›ì´ ë˜ê³  ì‹¶ìŠµë‹ˆë‹¤.
"""

# âœ… ì‹¤í–‰
mean_score = calculate_similarity(resume, job_posting, mode='mean')
weighted_score, debug_info = weighted_average_similarity_debug(resume, job_posting)

print(f"[í‰ê·  ê¸°ë°˜ ìœ ì‚¬ë„] {mean_score:.4f}")
print(f"[ê°€ì¤‘ í‰ê·  ê¸°ë°˜ ìœ ì‚¬ë„] {weighted_score:.4f}\n")

# âœ… ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
for i, (res, job, sim, weight) in enumerate(debug_info):
    print(f"ğŸ”¹ Pair {i+1}")
    print(f"   - ì´ë ¥ì„œ ë¸”ë¡: {res}")
    print(f"   - ì±„ìš©ê³µê³  ë¸”ë¡: {job}")
    print(f"   - ìœ ì‚¬ë„: {sim:.4f}")
    print(f"   - ê°€ì¤‘ì¹˜(sim^2): {weight:.4f}\n")
#0.5204, 0.4680 # ê´€ë ¨ ì—†ëŠ” 
#0.63, 0.59 # ê´€ë ¨ ìˆëŠ” ê±°